
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I’m currently a postdoctoral researcher at Syracuse University in Tripti Bhattachrya’s Paleoclimate Dynamics Lab where I am using leaf wax biomarkers and isotope-enabled climate simulations to reconstruct tropical circulation from Earth’s recent past. My recent interests are broad but deeply rooted in using the geologic record of past climatological and ecological changes to better understand future global change. In the past, I have used microbial biomarkers, fossil-pollen abundances, leaf-wax biomarkers, and climate simulations to understand the patterns, causes, and legacies of past climate changes.\n","date":1664978278,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1664978278,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m currently a postdoctoral researcher at Syracuse University in Tripti Bhattachrya’s Paleoclimate Dynamics Lab where I am using leaf wax biomarkers and isotope-enabled climate simulations to reconstruct tropical circulation from Earth’s recent past.","tags":null,"title":"David Fastovich","type":"authors"},{"authors":["David Fastovich","James M. Russell","Shaun A. Marcott","John W. Williams"],"categories":["Paleoclimatology","Fossil-pollen","Climate model","Younger Dryas"],"content":"","date":1664978278,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664978278,"objectID":"2c7a88bd2fbf6670d7229af1a72bb0a3","permalink":"https://davidfastovich.github.io/publication/qsr_2022/","publishdate":"2022-10-05T09:57:58-04:00","relpermalink":"/publication/qsr_2022/","section":"publication","summary":"Here we seek to establish the spatial fingerprints of precipitation and temperature changes in eastern North America during the Younger Dryas and explore the role of meltwater forcing in producing this pattern. Our analyses integrate a network of 42 fossil pollen records and 27 other hydroclimate proxy records, three AOGCM experiments with an imposed freshwater forcing, and the TraCE-21ka transient deglacial simulation. A recent synthesis of proxy-based temperature reconstructions suggests that Younger Dryas temperature reversals were limited to sites north of ~35 degrees N, while southern sites experienced either steady warming or a temperature maximum. Proxy records suggest a tripole precipitation pattern during the Younger Dryas and earlier Heinrich events, with the northeastern United States and Florida wetting, while sites from the Great Lakes Region to the Carolinas were dry. Of the AOGCMs analyzed, TraCE-21ka simulates Younger Dryas mean-field temperature and precipitation changes with the most skill but does not simulate warming in the southeastern United States. The likely role of meltwater forcing in producing subregional warming is indicated by the hosing-experiment AOGCMs, which consistently indicate localized warming and tripole precipitation anomalies, but the reconstructed and simulated patterns are poorly aligned, resulting in moderate model skill of mean-field temperature anomalies and negative skill in simulating past fingerprints. The reconstructed tripole helps reconcile prior apparent discrepancies in proxy records about whether eastern North America was wetter or dryer during the Younger Dryas. Although the hosing-experiment AOGCMs simulate the fingerprints poorly, they show that meltwater forcing can produce subregional warming and wetting in the eastern United States. This is due to enhanced northward heat transport from the Gulf of Mexico induced by a geostrophic adjustment of the midlatitude jet to cooling in the North Atlantic.","tags":["Paleoclimatology","Fossil-pollen","Climate model","Younger Dryas"],"title":"Spatial fingerprints and mechanisms of precipitation and temperature changes during the Younger Dryas in eastern North America","type":"publication"},{"authors":["David Fastovich"],"categories":["Python","Jupyter","matplotlib","NumPy","Pandas","xarray"],"content":"Introduction Now that we have Python setup within Anaconda, let’s use it for some science! Here, we will plot data using matplotlib and xarray, which will require data wrangling using NumPy, and pandas. Through this process we will build a skillset fundamental to (geo)data science that can be applied to a variety of fields outside of the geosciences.\nIf you’re following along in this tutorial series, there’s no need to install any Python modules in Anaconda as the previous tutorial made sure that the environment we created had the entire scientific Python stack installed. So, let’s open up Jupyter Lab and create a new Jupyter Notebook to hold the code that we’ll be writing today.\nTime series plotting with matplotlib Within our notebook we’ll need to call the three packages that we need and this can be accomplished with this code chunk:\nimport matplotlib.pyplot as plt import numpy as np import pandas as pd Within this code, we call the matplotlib Pyplot module into the Python instance as plt, so we will access all functions by including the prefix plt. and then typing in the function we want (e.g. plt.scatter for a scatterplot or plt.plot for a line plot). This same syntax is used to import NumPy and pandas and their associated functions.\nWith the modules we need in place, let’s ingest a timeseries of global mean annual temperature through time and plot it.\nFirst let’s start ingesting the csv by calling pd.read_csv and providing it with the URL of the HadCRUT5 time series. Note, that pandas has many functions and can readily ingest data from Excel and text data without a problem.\n# Read in csv from the HadCRUT5 dataset hadcrut5 = pd.read_csv(\u0026#34;https://www.metoffice.gov.uk/hadobs/hadcrut5/data/current/analysis/diagnostics/HadCRUT.5.0.1.0.analysis.summary_series.global.annual.csv\u0026#34;) # Lets take a look at the first and last 5 rows of the data using pd.head() # This tells us what the data looks like and the column names which we need to plot the data hadcrut5.head With the dataset imported and stored as the hadcrut5 variable, lets plot it. But first, lets get some terminology out the way. matplotlib creates “figures” that are composed of “subplots”. This structure is very powerful as it allows you to stitch together any plots you want within the same figure, as long as they’re separate subplots. Here, we will be creating a single figure composed of a single subplot which is apparent in the code. For a thorough explanation of how to use matplotlib to create more complicated visualizations see this guide. Since we are only making a single time series plot, this code can be shortened quite a bit, but I’ve chosen to leave it verbose to get you thinking about matplotlib plots as a collection of subplots because that’s a key feature of matplotlib.\n# Create figure instance fig = plt.figure(figsize = (6, 6)) # Figure size width, height in inches. # Add subplot to figure instance and name it ax ax = fig.add_subplot(111) # 111 means \u0026#34;1x1 grid, first subplot\u0026#34; # Plot ax.plot(hadcrut5[\u0026#39;Time\u0026#39;], hadcrut5[\u0026#39;Anomaly (deg C)\u0026#39;]) Notice how we call the ‘Time’ and ‘Anomaly (deg C)’ columns that we uncovered by running hadcrut5.head using brackets next to the pandas data frame variable name that we assigned earlier. The same can be accomplished by typing in hadcrut5.Time, but using the bracket notation is preferable in case you have column names with spaces, as in \u0026#39;Anomaly (deg C)\u0026#39;.\nOur plot still look’s a bit rough so lets polish it by adding axis labels and a title:\n# Create figure instance fig = plt.figure(figsize = (6, 6)) # Figure size width, height in inches. # Add subplot to figure instance and name it ax ax = fig.add_subplot(111) # 111 means \u0026#34;1x1 grid, first subplot\u0026#34; # Plot ax.plot(hadcrut5[\u0026#39;Time\u0026#39;], hadcrut5[\u0026#39;Anomaly (deg C)\u0026#39;]) # Subplot title ax.set_title(\u0026#39;Global Mean Annual Temperature Anomaly\u0026#39;) # X-axis label ax.set_xlabel(\u0026#34;Year\u0026#34;) # Y-axis label ax.set_ylabel(\u0026#34;Temperature Anomaly (°C)\u0026#34;) The plot is looking quite a bit better but we can still improve it by removing some whitespace which will require modifying some existing code that we have. Namely, the line that we call the figure instance.\n# Notice how we\u0026#39;ve added the \u0026#39;constrained_layout\u0026#39; argument and set it to true fig = plt.figure(figsize = (6, 6), constrained_layout=True) # Add subplot to figure instance and name it ax ax = fig.add_subplot(111) # 111 means \u0026#34;1x1 grid, first subplot\u0026#34; # Plot ax.plot(hadcrut5[\u0026#39;Time\u0026#39;], hadcrut5[\u0026#39;Anomaly (deg C)\u0026#39;]) # Subplot title ax.set_title(\u0026#39;Global Mean Annual Temperature Anomaly\u0026#39;) # X-axis label ax.set_xlabel(\u0026#34;Year\u0026#34;) # Y-axis label ax.set_ylabel(\u0026#34;Temperature Anomaly (°C)\u0026#34;) The plot is almost perfect, however the automatic x-ticks generated by matplotlib make it seem as the though the data extends to the year 2025 when it ends at the year 2022. Changing this portion of the plot will require using NumPy to produce a 1-dimensional array (1D vector) of the x-ticks that we want. We can make the process a bit easier by making use of a handy …","date":1664964000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664964000,"objectID":"46762f67b25c222186d29b9897883149","permalink":"https://davidfastovich.github.io/post/pt_2_matplotlib_numpy_pandas/","publishdate":"2022-10-05T10:00:00Z","relpermalink":"/post/pt_2_matplotlib_numpy_pandas/","section":"post","summary":"Learning the fundamentals of using pandas, NumPy, and matplotlib to read, manipulate, and plot data.","tags":["Python","Jupyter","matplotlib","NumPy","Pandas"],"title":"Part 2 - Data Ingest and Plotting","type":"post"},{"authors":["David Fastovich"],"categories":["Python","Jupyter"],"content":"Open science is becoming the standard for reproducible science and many fields have coalesced around several coding languages in support of this. R and Python have seen tremendous use in the academy and for good reason: both are free and open-source with endless packages that extend core functionality that are commonly well documented. This series will work through everything you’ll need to get up running with Python and even begin doing some basic data manipulation and statistics. In Part 1, we’re going to focus on getting Python installed and running, which we will verify by using Python as a calculator.\nThere are several ways to install Python and all have their advantages and disadvantages. I’ve found that using Anaconda or miniconda tends to be the easiest and the most user friendly. Anaconda has the advantage of having a graphical user interface, rather than relying on the command line making it a fantastic entry point for beginners. What’s the difference between simply going to Python’s website and installing that version? In terms of the Python installation, very little. However, in practice Anaconda solves many quality of life issues that plain Python installs have. For instance:\nAren’t comfortable with using command line tools? No problem! Anaconda has a complete GUI, even for installing packages. Want to install a package? Anaconda commonly installs packages a bit faster with less of a headache. Want to install a different version of Python? It’s a breeze with Anaconda! Just create a new Python environment and specify which version of Python you’d like to use. Want to do spatial data analysis? Gone are the days of dealing with managing to get Python to talk to an OGR/GDAL installation. Anaconda will install Python, OGR/GDAL, and make sure they’re communicating nicely with one another (this is also a huge boon if you use R). For everything that Anaconda brings to the table there are several downsides. The biggest issue seems to be installing Python modules that are not in the Anaconda repository. If you run into a Python module that is not hosted by Anaconda, you can still install it using pip in the command line, but this is usually not recommended. Anaconda likes to manage it’s own dependencies and using pip can break Python environments because Anaconda and pip don’t talk to one another too well. This means that pip may install a version of a module that doesn’t agree with other installed modules. In practice, I’ve found this issue to be quite rare, but it does come up occasionally and there are usually ways around it.\nWith the background out the way let’s walk through the steps we need to install Anaconda, Python within it, and then write our first Python script.\nStep 1 - Download Anaconda Follow hyperlink at right to download Anaconda.\nStep 2 - Install Anaconda Double click on the .exe (Windows) or .pkg (MacOS) file and follow the prompts to install. No need to make any changes here - the default install settings and location are all okay to use (i.e. just click okay a bunch of times).\nStep 3 - Getting Comfortable with Anaconda Navigator Anaconda automatically creates a base environment with Python and many common tools preinstalled. However, its good practice to create new environments for different projects to ensure that dependencies between modules don’t break code for different projects. For the purposes of this tutorial, we’ll use the base environment that Anaconda creates. However, if you ever create a new Python environment be aware that it comes with minimal modules preinstalled. When I create new environments I typically install the scientific Python stack as it contains the most commonly used functions you will need (NumPy, SciPy, matplotlib, IPython, pandas, SymPy, Jupyter Lab). Though not formally a part of the scientific Python stack I’d recommend also installing Spyder for a good integrated development environment for Python that is an excellent alternative to Jupyter Lab.\nTo open Anaconda, search for “Anaconda Navigator” in the Start Menu on a Windows Computer or Spotlight on a Mac.\nThis is the screen that will greet you when you open Anaconda Navigator:\nLet’s familiarize ourselves with the layout. The currently active environment is listed to the right of “Applications on” and environments can be easily changed by clicking on the dropdown menu. What we see here are the pre-installed modules and programs in the “base (root)” environment. To view all existing environments, add modules, and manage environments you would click on the “Environments” tab where you should only see a single entry for the “base (root)” environment. We’ll save this for another tutorial, so for now let’s open up the integrated development environment we’ll be using today, Jupyter Lab.\nStep 4 - You’re First Script! Let’s use Jupyter Lab to write our first Python script and plot some data!\nGo back to the “Home” page by clicking on the navigation menu on the left had side of Anaconda Navigator and scroll …","date":1664928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664928000,"objectID":"f6cd029c8dd80694f4921584cd381a73","permalink":"https://davidfastovich.github.io/post/pt_1_python_install/","publishdate":"2022-10-05T00:00:00Z","relpermalink":"/post/pt_1_python_install/","section":"post","summary":"Let's get started with Python by installing it!","tags":["Python","Jupyter"],"title":"Part 1 - Installing Python","type":"post"},{"authors":["David Fastovich"],"categories":["Paleoclimatology","Fossil-pollen","brGDGT","Biodiversity","Climate Model"],"content":"","date":1659189478,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659189478,"objectID":"5c21063f03f57ffc4ceb5f89ce80a6bd","permalink":"https://davidfastovich.github.io/publication/diss_2022/","publishdate":"2022-07-30T09:57:58-04:00","relpermalink":"/publication/diss_2022/","section":"publication","summary":"Increasing global temperatures from anthropogenic greenhouse gas emissions are driving widespread climatological and ecological changes globally. Abrupt global changes that share rates of climate change similar to those experienced today (Overpeck et al. 2003; Williams and Burke 2019) are recorded throughout the geologic record and offer important insights that can help predict future anthropogenic change. The Deglacial period (19,000 to 11,000 years before present) after the Last Glacial Maximum has been a key interval for understanding ecological and climatological responses to increasing greenhouse gas concentrations and a warming climate (COHMAP Project Members 1988; Nolan et al. 2018; Mottl et al. 2021). Imposed on this gradual warming are abrupt climate oscillations that onset within decades to centuries, last for millennia, and are commonly attributed to changes in the Atlantic meridional overturning circulation forced by the input of freshwater into the North Atlantic Ocean. The most recent of these millennial-scale climate events is the Younger Dryas (ca. 12,900 to 11,700 years before present) and caused spatially complex climate changes globally. In this dissertation, we first aim to determine the spatial patterns of climate change and the atmospheric mechanisms responsible for driving abrupt climate change regionally in eastern North America through the use of organic temperature biomarkers (brGDGTs) and climate models. Second, we seek to disentangle the contributions of glacial and millennial-scale climate variability upon modern patterns of species richness in eastern North America.\nChapter 2 seeks to determine the spatial fingerprint of Younger Dryas temperature changes in eastern North America. We develop a spatially dense multiproxy network of temperature reconstructions relying upon statistical transfer functions applied to fossil-pollen abundances and an independent proxy, based on organic biomarkers (brGDGTs). This analysis indicates that temperature changes during the Younger Dryas followed a dipole pattern in eastern North America. Temperatures lowered abruptly in maritime Canada and the northeastern United States nearly synchronously with temperature records from Greenland (Severinghaus et al. 1998). Cooling is also reconstructed in the Great Lakes region but delayed by ~400 years. Sites south of 35°N exhibited an antiphased response and lack YD cooling, with Florida sites indicating a thermal maximum. Warming in Florida during the Younger Dryas suggests that the ‘bipolar-seesaw’ conceptualization is an oversimplification of the spatial patterns of global climate changes. Focus must be placed on constraining regional climate changes to refine the mechanisms of abrupt climate change. Chapter 3 aims to better understand the atmospheric mechanisms for these antiphased temperature changes in eastern North America. We accomplish this by combining our multiproxy temperature network with a synthesis of hydroclimate reconstructions to compare against four climate models with meltwater hosing experiments that resemble the onset of the Younger Dryas. Precipitation changes followed a tripole pattern with wetting in the northeastern United States and Florida and drying from the Great Lakes region to the Carolinas, in contrast to the temperature dipole resolved in Chapter 2. Analysis of the climate models highlights the dual role of ice sheets and meltwater-induced weakening of the Atlantic meridional overturning circulation as the key drivers of the reconstructed warming and wetting in the southeastern United States. Reduced northward oceanic heat transport in the Atlantic Ocean increased the latitudinal temperature gradient and strengthened the jet stream, leading to upper-level divergence over eastern North America and the transport of warmer and moister air into the southeastern United States. For Chapter 4, we use our multiproxy temperature and precipitation reconstruction from prior chapters, alongside 11 climate simulations of millennial-scale climate events forced by meltwater pulses, to assess whether legacies of these climate changes can be detected in the contemporary diversity of amphibians, birds, mammals, reptiles, and trees in eastern North America. Generalized additive models that use both contemporary and paleoclimatic predictors suggest that past millennial scale climate oscillations have left an imprint on contemporary amphibian and arboreal biodiversity, though the exact role of past climate changes remains uncertain. Generalized additive models that use the multiproxy network of Younger Dryas climate reconstructions and a subset of the climate models analyzed suggest that greater millennial scale climate variability is predictive of greater contemporary biodiversity. However, generalized additive models that use four of the climate models suggest that millennial-scale climate stability is predictive of greater contemporary richness in eastern North America. Disagreement in the sign, magnitude, and spatial fingerprint of climate changes among the 11 climate simulations and the multiproxy climate reconstructions precludes further refining the role of millennial-scale climate oscillations at this time. This uncertainty highlights that caution should be used when attempting to model contemporary biodiversity based on individual paleoclimatic simulations. Higher resolution climate simulations forced with accurate boundary conditions are necessary to constrain the relationship between past millennial-scale climate changes and contemporary biodiversity.","tags":["Paleoclimatology","Fossil-pollen","brGDGT","Biodiversity","Climate Model"],"title":"Patterns, mechanisms, and legacies of abrupt climate change: examining the Younger Dryas in eastern North America","type":"publication"},{"authors":["David Fastovich"],"categories":["Python","xarray","Jupyter","Binder"],"content":"During my dissertation research, I began playing with climate model output, which ended up integral in two of my three chapters. None of the work in the dissertation would have been possible without tools such as xarray which made working with climate model data as painless as can possibly be. I’m writing this blog post in 2022, and I originally wrote this script for a lab meeting in 2021 meant to demonstrate why Jupyter Notebooks are so great, however, I think the introduction to netCDFs (the default climate model format) and xarray are the real strengths of the code below. My lab at the time was invested in the R ecosystem, so the goal of the script below was to demonstrate how to leverage the strengths of two coding languages within the same script. If you follow along with the script below and have an introductory understanding of R and Python here’s what you’ll end up learning:\nnetCDF file structure Opening and manipulating netCDF files in Python using xarray Plot data from netCDF files in Python using matplotlib Pass data from pandas in Python to R Plot using ggplot2 in R If following along with the script isn’t your cup of tea, feel free to open the script and actually run the code through Binder! Just click on the icon below and wait a few minutes for the Binder environment to build. From there, you’ll be able to run and edit the code for yourself in JupyterLab! If you’re unfamiliar with JupyterLab, wait for an upcoming blog post that introduces JupyterLab and how to use it for scientific computing - I’ll be sure to link it here once it’s up.\n","date":1631145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631145600,"objectID":"a01bdbaae46195a128826979daf05389","permalink":"https://davidfastovich.github.io/post/xarray_intro/","publishdate":"2021-09-09T00:00:00Z","relpermalink":"/post/xarray_intro/","section":"post","summary":"Python is a fantastic tool for handling meteorological and climate model output. Here, I go over the basics of climate analysis workflows in Python, with a twist at the end.","tags":["Python","xarray","Jupyter","Binder"],"title":"Python, Jupyter Notebooks, and xarray introduction","type":"post"},{"authors":["David Fastovich"],"categories":["R","neotoma","Jupyter","Binder"],"content":"Making research data and code publicly available is core to making science more transparent and reproducible, yet more can be done towards this goal. Researchers commonly share their code and data through various academic repositories like Zenodo or public code repositories like GitHub, however, there remains a limitation to easy reproducibility: creating the computing environment that produced the code. The computing environment encompasses everything about the computer that was used to produce the final analyses and figures from the operating system to the coding language version and packages used for analyses. This is where Binder comes in - this is a free service that creates a virtual computer in the cloud based on the specifications listed in a GitHub repository. In the process of building the environment, Binder also clones the GitHub repository making it painless to reproduce analyses. From an end user’s perspective, this means that GitHub repositories that link to Binder environments allow someone to view the code, click a single button, and within minutes begin running that code on a computer in the cloud. Binder is particularly well suited for scientific computing done using R and Python because these code languages are freely accessible.\nThis introduction will teach you how to build an R Binder environment that can be used through Jupyter Notebooks or RStudio. This guide assumes that you have some experience with GitHub and are able to build your own public code repository. Once you have you’re GitHub repository live (we’ll use this introduction to the Neotoma Paleoecology Database I wrote some time back as an example), you need to a couple of files to the repository. First, you need to add a file named runtime.txt to the root of the GitHub repository with a single like that specifies the R version you would like Binder to install. In the example, I ask Binder to use R version 3.6 released on 2019-04-12 but adding this single line to the text file:\nr-3.6-2019-04-12 Important note - Binder builds R environments from MRAN, the Microsoft R Application Network, which is usually a few releases behind CRAN, The Comprehensive R Archive Network. So be sure to use a version of R that’s hosted on MRAN.\nAfter that, if your code has any package dependencies, you specify these to install in a file named install.R that is located in the root of the GitHub repository. Using the previous example, the install.R file has the following lines to install required packages:\ninstall.packages(\u0026#34;tidyverse\u0026#34;) install.packages(\u0026#34;rmarkdown\u0026#34;) install.packages(\u0026#34;httr\u0026#34;) install.packages(\u0026#34;shinydashboard\u0026#34;) install.packages(\u0026#34;leaflet\u0026#34;) install.packages(\u0026#34;neotoma\u0026#34;) install.packages(\u0026#34;rioja\u0026#34;) install.packages(\u0026#34;Bchron\u0026#34;) From here, all you need to do is create a live link to Binder in a markdown file on the root of the GitHub repository. The structure of the link follows a simple syntax where:\n[![Binder](https://mybinder.org/badge_logo.svg)] is markdown for “link this text inside of this badge image”, and\n(https://mybinder.org/v2/gh/davidfastovich/R-Jupyter-Tutorial/main?urlpath=lab) tells Binder what repository to download and what program to open it in. Here, https://mybinder.org/v2/gh/ links to Binder v2 and specifies that it should look at a GitHub repository (gh), from user davidfastovich, titled R-Jupyter-Tutorial. The last bit, /main?urlpath=lab tells Binder to open JupyterLab on launch, but this can be configured to open RStudio by changing it to main?urlpath=rstudio. So, the line in the markdown file would read either\n![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/davidfastovich/R-Jupyter-Tutorial/main?urlpath=lab) for JupyterLab or\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/davidfastovich/R-Jupyter-Tutorial/main?urlpath=rstudio) for RStudio.\nAnd that’s all there is to it! This is quite a lot of text for a simple process: you create three files on the root of the GitHub repository and link to Binder. In all, this should take ~5 minutes and can greatly extend open science!\nBelow are two links that will launch the example repository in Binder in JupyterLab or RStudio:\nJupyterlab RStudio Hopefully, playing with code within Binder has demonstrated its use for open science, but also demonstrated that Binder can be a powerful educational tool. I’ve taken many coding classes and the first barrier to entry was getting the coding environment set up on a personal computer. Binder resolves that issue entirely - students don’t have to worry about installing or upgrading anything, it’s all handled by the instructor.\n","date":1631145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631145600,"objectID":"32ba5260935081d2355a84650b6b2ef5","permalink":"https://davidfastovich.github.io/post/r_jupyter/","publishdate":"2021-09-09T00:00:00Z","relpermalink":"/post/r_jupyter/","section":"post","summary":"Reproducible science is best facilitated by open science. Using open source tools like R is great and can be strengthened by writing and sharing reproducible code through GitHub and Binder.","tags":["R","neotoma","Jupyter","Binder"],"title":"Using R in Jupyter Notebooks and Binder","type":"post"},{"authors":["Allison M. Jensen","David Fastovich","Benjamin I. Watson","Jacqueline L. Gill","Stephen T. Jackson","James M. Russell","Joseph Bevington","Kate Hayes","Katherine B. Lininger","Claire Rubbelke","Grace C. Schellinger","John W. Williams"],"categories":["Charcoal","Abrupt Ecological Change","Fossil-pollen","brGDGT"],"content":"","date":1612015078,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612015078,"objectID":"59720c82a0e74498d03ec5170a7b5ad3","permalink":"https://davidfastovich.github.io/publication/joe_2021/","publishdate":"2022-07-30T09:57:58-04:00","relpermalink":"/publication/joe_2021/","section":"publication","summary":"In the southern Great Lakes Region, North America, between 19,000 and 8,000 years ago, temperatures rose by 2.5-6.5 degrees C and spruce Picea forests/woodlands were replaced by mixed-deciduous or pine Pinus forests. The demise of Picea forests/woodlands during the last deglaciation offers a model system for studying how changing climate and disturbance regimes interact to trigger declines of dominant species and vegetation-type conversions. The role of rising temperatures in driving the regional demise of Picea forests/woodlands is widely accepted, but the role of fire is poorly understood. We studied the effect of changing fire activity on Picea declines and rates of vegetation composition change using fossil pollen and macroscopic charcoal from five high-resolution lake sediment records. The decline of Picea forests/woodlands followed two distinct patterns. At two sites (Stotzel-Leis and Silver Lake), fire activity reached maximum levels during the declines and both charcoal accumulation rates and fire frequency were significantly and positively associated with vegetation composition change rates. At these sites, Picea declined to low levels by 14 kyr BP and was largely replaced by deciduous hardwood taxa like ash Fraxinus, hop-hornbeam/hornbeam Ostrya/Carpinus and elm Ulmus. However, this ecosystem transition was reversible, as Picea re-established at lower abundances during the Younger Dryas. At the other three sites, there was no statistical relationship between charcoal accumulation and vegetation composition change rates, though fire frequency was a significant predictor of rates of vegetation change at Appleman Lake and Triangle Lake Bog. At these sites, Picea declined gradually over several thousand years, was replaced by deciduous hardwoods and high levels of Pinus and did not re-establish during the Younger Dryas. Synthesis. Fire does not appear to have been necessary for the climate-driven loss of Picea woodlands during the last deglaciation, but increased fire frequency accelerated the decline of Picea in some areas by clearing the way for thermophilous deciduous hardwood taxa. Hence, warming and intensified fire regimes likely interacted in the past to cause abrupt losses of coniferous forests and could again in the coming decades.","tags":["Charcoal","Abrupt Ecological Change","Fossil-pollen","brGDGT"],"title":"More than One Way to Kill a Spruce Forest: The Role of Fire and Climate in the Late-Glacial Termination of Spruce Woodlands across the Southern Great Lakes","type":"publication"},{"authors":["David Fastovich","James M. Russell","Stephen T. Jackson","Teresa R. Krause","Shaun A. Marcott","John W. Williams"],"categories":["Paleoclimatology","Fossil-pollen","brGDGT","Bayesian statistics"],"content":"","date":1596117478,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596117478,"objectID":"b3ed473fe08320f548c6136c387e2e4b","permalink":"https://davidfastovich.github.io/publication/grl_2020/","publishdate":"2022-07-30T09:57:58-04:00","relpermalink":"/publication/grl_2020/","section":"publication","summary":"Abstract The Younger Dryas (YD, 12.9-11.7 ka) is the most recent, near-global interval of abrupt climate change with rates similar to modern global warming. Understanding the causes and biodiversity effects of YD climate changes requires determining the spatial fingerprints of past temperature changes. Here we build pollen-based and branched glycerol dialkyl glycerol tetraether-based temperature reconstructions in eastern North America (ENA) to better understand deglacial temperature evolution. YD cooling was pronounced in the northeastern United States and muted in the north central United States. Florida sites warmed during the YD, while other southeastern sites maintained a relatively stable climate. This fingerprint is consistent with an intensified subtropical high during the YD and demonstrates that interhemispheric responses were more complex spatially in ENA than predicted by the bipolar seesaw model. Reduced-amplitude or antiphased millennial-scale temperature variability in the southeastern United States may support regional hotspots of biodiversity and endemism.","tags":["Paleoclimatology","Fossil-pollen","brGDGT","Bayesian statistics"],"title":"Spatial Fingerprint of Younger Dryas Cooling and Warming in Eastern North America","type":"publication"},{"authors":["David Fastovich","James M. Russell","Stephen T. Jackson","John W. Williams"],"categories":["Paleoclimatology","Fossil-pollen","Bayesian statistics"],"content":"","date":1582639078,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582639078,"objectID":"95a86712b29ef08f8d4c5eb10a6ddd55","permalink":"https://davidfastovich.github.io/publication/qsr_2020/","publishdate":"2022-07-30T09:57:58-04:00","relpermalink":"/publication/qsr_2020/","section":"publication","summary":"Understanding the drivers of vegetation dynamics and no-analog communities in eastern North America is hampered by a scarcity of independent temperature indicators. We present a new branched glycerol dialkyl glycerol tetraether (brGDGT) temperature record from Bonnet Lake, Ohio (18 - 8 ka) and report uncertainty estimates based on Bayesian linear regression and bootstrapping. We also reanalyze a previously published brGDGT record from Silver Lake, Ohio, using improved chromatographic methods. All pollen- and brGDGT-based temperature reconstructions showed qualitatively similar deglacial trends but varying magnitudes. Separating 5- and 6- methyl brGDGTs resulted in substantially lower estimates of deglacial temperature variations (6.4 degrees C) than inferred from earlier brGDGT methods and pollen (11.8 degrees C, 12.0 degrees C respectively). Similar trends among proxies suggest good fidelity of brGDGTs to temperature, despite calibration uncertainties. At both sites, the rise and decline of no-analog communities closely track brGDGT-inferred temperatures, with a lag of 0-150 years. The timing of temperature and ecological events varies between Bonnet and Silver Lakes, likely due to age model uncertainties. Climate sensitivity analyses indicate a linear sensitivity of vegetation composition to temperature variations, albeit noisy and significant only with a 500-year bin. The formation of no-analog plant communities in the upper Midwest is closely linked to late-glacial warming, but other factors, such as temperature seasonality or end-Pleistocene megafaunal extinctions, remain viable.","tags":["Paleoclimatology","Fossil-pollen","Bayesian statistics"],"title":"Deglacial Temperature Controls on No-Analog Community Establishment in the Great Lakes Region","type":"publication"},{"authors":["David Fastovich"],"categories":["Paleoclimatology","Fossil-pollen","brGDGT"],"content":"","date":1530367078,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530367078,"objectID":"1f5f8fb0b7b16cb6fba80d664e1e0004","permalink":"https://davidfastovich.github.io/publication/ms_2018/","publishdate":"2022-07-30T09:57:58-04:00","relpermalink":"/publication/ms_2018/","section":"publication","summary":"Temperature reconstructions in eastern North America from pollen are rife in the literature, but these reconstructions cannot be used to study the sensitivity and response time of vegetation to temperature changes. Branched glycerol dialkyl glycerol tetraethers (brGDGT) have recently been applied to reconstruct temperature at Silver Lake, OH, where the resulting temperature estimates closely tracked a regional pollen temperature reconstruction for the southern Great Lakes region. Here, we present the second brGDGT temperature record in this region, at Bonnet Lake, OH, using a new method for brGDGT detection and four alternative calibration functions, and reanalyze sediments from Silver Lake. We compare results across brGDGT detection methods, between sites and among calibration functions, and assess calibration uncertainty using a Bayesian linear regression. Of the calibration functions, MAT MBT′5Me reproduced the regional pollen stack and existing temperature record from Silver Lake most closely but is ~2°C warmer on average than the existing brGDGT temperature record. In the Bayesian regression analyses, the 95% credible interval when using the calibration from Weijers et al. (2007) was +/- 12.3°C. The MAT MBT′5Me calibration from De Jonge et al. (2014) had a smaller uncertainty (+/- 9.8°C 95% credible interval), likely resulting from a reduced sensitivity of brGDGT methylation to pH. These uncertainties are based only on analyses of the calibration data and likely overestimate reconstruction uncertainty, because calibration soil samples are heterogenous at the sampling scale and are at locations distant from the corresponding temperature measurement. Uncertainty can be reduced by creating improved calibration datasets of modern lake sediments. Despite uncertainty in absolute temperature and range, climatic trends are closely similar among brGDGT reconstructions and provide insight into the temperature drivers of past vegetation dynamics. Picea decline begins shortly after the start of Bølling-Allerød warming. The establishment and disappearance of no-analog communities lag temperature change by 200-500 years at Bonnet Lake and Silver Lake. The lack of synchrony in the timing of warming and no-analog community establishment at Bonnet Lake and Silver Lake, but agreement in the sequence of events, indicates that local climate is an important control on vegetation assemblages. The extension of brGDGTs to the Great Lakes Region is further supported in this study, but uncertainty in temperature estimates emphasizes the need for improved calibration datasets.","tags":["Paleoclimatology","Fossil-pollen","brGDGT"],"title":"Temperature Controls on No-Analog Community Establishment in the Great Lakes Region","type":"publication"}]